\ProvidesFile{lecture23.tex}[Лекция 23]


\subsection{Матрица билинейной формы}

При изучении любого объекта один из первых вопросов: <<а как этот объект задавать?>> Сейчас мы коснемся этого вопроса для билинейных форм и начнем со следующего.

\begin{definition}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма,  $e_1,\ldots,e_n\in V$ -- базис пространства $V$ и $f_1,\ldots,f_m\in U$ -- базис пространства $U$.
Тогда матрица $B_\beta$ с коэффициентами $b_{ij} = \beta(e_i,f_j)$ называется матрицей билинейной формы $\beta$ в паре базисов $e_1,\ldots,e_n$ и $f_1,\ldots,f_m$.
\end{definition}

\begin{claim}
\label{claim::BilinearBasis}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма,  $e = (e_1,\ldots,e_n)$ -- базис пространства $V$ и $f=(f_1,\ldots,f_m)$ -- базис пространства $U$.
Пусть $v = ex$, $x\in F^n$, $u =fy$, $y\in F^m$ и $B$ -- матрица билинейной формы $\beta$ в базисах $e$ и $f$.
Тогда $\beta(v,u) = x^t B y$.
\end{claim}
\begin{proof}
Действительно, 
\[
\beta(v,u) = \beta(\sum_{i=1}^n x_i e_i, \sum_{j=1}^m y_j f_j) = \sum_{i,j} x_iy_j\beta(e_i, f_j) = x^t B y
\]
\end{proof}

Таким образом, когда вы работаете с парой пространств $V$ и $U$, после выбора базиса они превращаются в $F^n$ и $F^m$, соответственно, а билинейная форма $\beta\colon V\times U\to F$ превращается в отображение $\beta\colon F^n \times F^m \to F$ по правилу $(x,y)\mapsto x^t B y$.

\begin{claim}
\label{claim::BilinearMatrices}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма,  $e = (e_1,\ldots,e_n)$ -- базис пространства $V$ и $f=(f_1,\ldots,f_m)$ -- базис пространства $U$.
Тогда отображение $\operatorname{Bil}(V,U)\to \operatorname{M}_{n\,m}(F)$ по правилу $\beta\mapsto B_\beta$ является биекцией.
\end{claim}
\begin{proof}
Из утверждения~\ref{claim::BilinearBasis} следует, что $\beta(x,y) = x^t B_\beta y$.
Значит, билинейная форма восстанавливается по своей матрице и отображение $\beta\mapsto B_\beta$ инъективно.
Обратно, если $B\in \operatorname{M}_{n\,m}(F)$ -- произвольная матрица, то рассмотрим форму $\beta(x,y) = x^t B y$.
Тогда по определению $B = B_\beta$.
\end{proof}

\paragraph{Матричный формализм}

Пусть $\beta\colon V\times U\to F$ -- билинейная форма и пусть $v = (v_1,\ldots,v_s)$ -- некоторый набор векторов из $V$ и $u = (u_1,\ldots,u_t)$ -- набор векторов из $U$.
Тогда рассмотрим следующую конструкцию
\[
v^t\cdot_\beta u = 
\begin{pmatrix}
{v_1}\\{\vdots}\\{v_s}
\end{pmatrix}
\cdot_\beta
\begin{pmatrix}
{u_1}&{\ldots}&{u_t}
\end{pmatrix}
=
\begin{pmatrix}
{v_1\cdot_\beta u_1}&{\ldots}&{v_1\cdot_\beta u_t}\\
{\vdots}&{\ddots}&{\vdots}\\
{v_s\cdot_\beta u_1}&{\ldots}&{v_s\cdot_\beta u_t}\\
\end{pmatrix}
\]
То есть мы умножаем столбец из векторов из $V$ на строку из векторов из $U$ с помощью билинейной формы, рассматриваемой как оператор умножения.
Тогда результат будет матрица из $\operatorname{M}_{s\,t}(F)$.
Причем умножение происходит по тем же самым формальным правилам, что и обычное матричное умножение, только с использованием $\cdot_\beta$ вместо обычного умножения (которое для векторов даже не определено).

Тогда, если выбрать $e = (e_1,\ldots,e_n)$ -- базис $V$ и $f = (f_1,\ldots,f_m)$ -- базис $U$, то 
\[
B_\beta = e^t\cdot_\beta f = 
\begin{pmatrix}
{e_1}\\{\vdots}\\{e_n}
\end{pmatrix}
\cdot_\beta
\begin{pmatrix}
{f_1}&{\ldots}&{f_m}
\end{pmatrix}
\]
Мы привыкли, что в случае линейных отображений вычисления можно вести в удобной матричной форме.
Последнее равенство позволяет вычисления с билинейными формами сводить к матричной.

\begin{claim}
Пусть $V$ и $U$ -- векторные пространства над полем $F$, $e_1,\ldots,e_n \in V$ -- базис $V$ и $f_1,\ldots,f_m\in U$ -- базис $U$.
Тогда для любого набора чисел $b_{ij}\in F$, где $1\leqslant i \leqslant n$ и $1\leqslant j \leqslant m$, существует единственная билинейная форма $\beta\colon V\times U\to F$ такая, что $\beta(e_i,f_j) = b_{ij}$.
\end{claim}
\begin{proof}
По сути -- это переформулировка утверждения~\ref{claim::BilinearMatrices}.
\end{proof}

\begin{claim}
Пусть $\beta\colon V\times U\to F$ -- билинейная форма.
Пусть в пространстве $V$ зафиксировано два базиса $e=(e_1,\ldots,e_n)$ и $e' = (e_1',\ldots,e_n')$ с матрицей перехода $C\in \operatorname{M}_n(F)$ такой, что $e'=eC$, пусть в пространстве $U$ также зафиксированы два базиса $f = (f_1,\ldots,f_m)$ и $f'=(f_1',\ldots,f_m')$ с матрицей перехода $D\in \operatorname{M}_m(F)$ такой, что $f' = fD$.
Если $B_\beta$ -- матрица $\beta$ в базисах $e$ и $f$ и $B_\beta'$ -- матрица $\beta$ в базисах $e'$ и $f'$, тогда $B_\beta' = C^t B_\beta D$.
\end{claim}
\begin{proof}
Пользуясь только что введенным формализмом можно проделать следующие вычисления:%
\footnote{Обратите внимание, что тут у нас присутствует два умножения: матричное с числами и матричное с билинейной формой.
Порядок этих операций (то есть расстановка скобок) не важны, это следует просто из определения билинейной формы, если присмотреться внимательно.}
\[
B_\beta' = (e')^t\cdot_\beta f' = (eC)^t \cdot_\beta (f D) = (C^t e^t) \cdot_\beta (f D) = C^t(e^t \cdot_\beta f) D = C^t B_\beta D
\]
\end{proof}

\paragraph{Замечания}

\begin{itemize}
\item Заметим, что если билинейная форма определена на одном пространстве $\beta\colon V\times V\to F$, то достаточно выбрать один базис $e=(e_1,\ldots,e_n)$, после чего коэффициенты $B_\beta$ считаются по правилу $b_{ij} = \beta(e_i,e_j)$.
При этом, если $e'=(e_1',\ldots,e_n')$ -- другой базис и $e'=eC$, где $C$ -- матрица перехода, то $B_\beta' = C^t B C$.

\item Пусть у нас есть два векторных пространства $V$ и $U$.
Тогда на них могут жить два разного рода объектов: линейные отображения и билинейные формы, например, $\phi\colon V\to U$ и $\beta\colon U\times V \to F$.
Если мы выберем базис в $V$ и базис в $U$, то $V$ превращается в $F^n$, а $U$ -- в $F^m$.
В этом случае, линейное отображение $\phi$ описывается некоторой матрицей $A\in\operatorname{M}_{m\,n}(F)$, при этом $\phi(x) = Ax$.
С другой стороны, билинейная форма тоже описывается матрицей $B\in \operatorname{M}_{m\,n}(F)$, при этом $\beta(x,y) = x^tBy$.

Таким образом, для описания и линейных отображений и билинейных форм в фиксированном базисе мы используем матрицы (причем одного и того же размера).
Возникает вопрос: <<а как понять,  когда матрица задает линейное отображение, а когда билинейную форму?>> Если  нам выдали только одну пару базисов и матрицу $S$, то ответ простой -- никак.
В фиксированном базисе они не отличимы.
Мы можем считать нашу матрицу $S$ линейным оператором или билинейной формой, в зависимости от наших предпочтений.
Однако, если нам выдали несколько базисов, например два, и в этих базисах наш объект задается матрицами $S$ и $S'$.
То отличить оператор от билинейной формы можно по формуле перехода, а именно, если задан оператор, то $S' = C^{-1}S D$, а если билинейная форма, то $S' = C^t S D$.
Конечно, если базисы трепетно подобраны (врагом или другом -- это как повезет), то мы все равно можем не заметить разницы.
Но если мы будем сравнивать во всех возможных базисах, то ответ определяется однозначно.
\end{itemize}

\subsection{Матричные характеристики билинейной формы}
\label{subsection::BilChar}

В случае линейного отображения или оператора мы поступали так: выбирали базисы (или базис) и задавали их матрицами.
Потом считали какие-то характеристики этих самых матриц и показывали, что они не зависят от базисов.
В случае линейных отображений между разными пространствами у нас по сути была только одна характеристика -- ранг.
Оказывается, что для билинейных форм на паре разных пространств это тоже корректная характеристика.

\paragraph{Ранг}

Пусть $\beta\colon V\times U\to F$ -- билинейная форма и в каких то парах базисов она задана матрицами $B$ и $B'$.
Тогда $B' = C^t B D$ для некоторых невырожденных матриц $C$ и $D$.
Тогда $\rk B' = \rk B$, так как он не меняется при умножении слева и справа на невырожденную матрицу (утверждение~\ref{claim::rkInvariance}).

\subsection{Ортогональные дополнения и ядра}

\begin{definition}
Пусть $\beta\colon V\times U\to F$ -- билинейная форма.
Тогда
\begin{itemize}
\item Если $X\subseteq V$ -- произвольное подмножество, тогда правое ортогональное дополнение к $X$ это
\[
X^\bot = \{u\in U \mid \beta(x,u) = 0\,\forall x\in X\} = \{u\in U\mid \beta(X,u) = 0\}
\]

\item Если $Y\subseteq U$ -- произвольное подмножество, тогда левое ортогональное дополнение к $Y$ это
\[
{}^\bot Y = \{v\in V\mid \beta(v, y) = 0\,\forall y\in Y\} = \{v\in V\mid \beta(v, Y) = 0\}
\]
\end{itemize}
\end{definition}

\paragraph{Замечания}

\begin{itemize}
\item
Когда понятно о чем идет речь и нет путаницы, обычно оба дополнения обозначают $X^\bot$ и $X^\bot$.
Обычно это не мешает если пространства $V$ и $U$ разные, так как каждое дополнение живет в своем отдельном пространстве.
Однако, если форма определена на одном пространстве $\beta\colon V\times V\to F$, то приходится использовать разные обозначения, так как для $X\subseteq V$ определены оба дополнения $X^\bot$ и ${}^\bot X$ и оба живут в $V$.
Я постараюсь различать дополнения там, где это необходимо.

\item 
Стоит отметить, что ортогональное дополнение $X^\bot$ к подмножеству $X\subseteq V$ обязательно будет подпространством в $U$, аналогично и для второго дополнения.
\end{itemize}


\paragraph{Пример}

Давайте выясним как считать левое и правое ортогональные дополнения к подпространству.
Пусть $\beta\colon F^n\times F^m \to F$ -- некоторая билинейная форма заданная $\beta(x,y) = x^t B y$, где $B\in \operatorname{M}_{n\,m}(F)$.
Пусть $W = \langle w_1,\ldots,w_s\rangle\subseteq F^n$ -- некоторое подпространство заданное в виде линейной оболочки.
Положим $T = (w_1|\ldots|w_s)\in\operatorname{M}_{n\,s}(F)$ -- матрица из столбцов $w_i$.
Тогда 
\[
W^\bot = \{y\in F^m \mid T^t By = 0\}
\]
Аналогично, если $U=\langle u_1,\ldots,u_r \rangle \subseteq F^m$ -- подпространство и $P = (u_1|\ldots|u_r)$ -- матрица из столбцов $u_i$.
То
\[
{}^\bot U = \{x\in F^n\mid x^t B P = 0\} = 
\{x\in F^n \mid P^t B^t x = 0\}
\]


\begin{definition}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма, тогда ее правым ядром называется $\ker^R \beta =V^\bot$, а левым ядром $\ker^L\beta = {}^\bot U$.
\end{definition}

Смысл левого ядра в том, что это такие векторы из $V$, которые на что из $U$ ни умножай, все равно получишь $0$.
В этом смысле -- это не интересные векторы, изучение которых с точки зрения билинейной форм невозможно.
Аналогично с правым ядром.

\paragraph{Пример}

Пусть $\beta\colon F^n\times F^m \to F$ -- некоторая билинейная форма заданная правилом $\beta(x,y) = x^t By$, где $B\in \operatorname{M}_{n\,m}(F)$.
Тогда
$\ker^R \beta = \{y\in F^m\mid By = 0\}$ и $\ker^L\beta = \{x\in F^n \mid x^t B = 0\} = \{x\in F^n \mid B^t x = 0\}$.

\begin{definition}
Билинейная форма $\beta\colon V\times U\to F$ называется невырожденной, если $\ker^R\beta = 0$ и $\ker^L\beta = 0$.
\end{definition}


\begin{claim}
Билинейная форма $\beta\colon V\times U\to F$ невырождена тогда и только тогда, когда $\dim V = \dim U$ и матрица формы $\beta$ невырождена.
\end{claim}
\begin{proof}
Пусть форма $\beta$ невырожденная.
Выберем базисы в $V$ и $U$, тогда наша билинейная форма превратится в $\beta\colon F^n \times F^m\to F$ по правилу $(x,y)\mapsto x^t B y$ для некоторой матрицы $B\in \operatorname{M}_{n\,m}(F)$.
Тогда
\[
\ker^R \beta = \{y\in F^m\mid By = 0\}\quad\text{и}\quad \ker^L\beta = \{x\in F^n \mid B^t x = 0\}
\]
Если размерности пространств разные, то матрица $B$ не квадратная и хотя бы одно из ядер не ноль, так как хотя бы одна из систем $B y = 0$ или $B^t x = 0$ содержит переменных больше чем уравнений, а значит есть ненулевое решение.
Теперь мы знаем, что $B$ квадратная и система $By = 0$ имеет только нулевые решения, значит $B$ -- невырожденная матрица по утверждению~\ref{claim::InvertibleDiscription}.

Обратно, пусть $\dim V = \dim U$ и матрица $B_\beta$ не вырождена.
Тогда в каких-то координатах $\beta$ записывается так $\beta\colon F^n \times F^n \to F$ по правилу $(x,y)\mapsto x^t By$.
Так как $B$ невырожденная, то системы $B y = 0$ и $B^t x = 0$ имеют только нулевые решения, значит оба ядра нулевые, значит форма невырожденная.
\end{proof}

\begin{claim}
\label{claim::BilinearKernels}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма, тогда:%
\footnote{Это утверждение является прямым аналогом утверждения~\ref{claim::ImKer} для линейных отображений и является очередным проявлением тривиального наблюдения для систем линейных уравнений, что количество главных и свободных переменных равно количеству всех переменных.}
\begin{enumerate}
\item $\dim\ker^L \beta + \rk \beta = \dim V$

\item $\dim\ker^R \beta + \rk \beta = \dim U$
\end{enumerate}
\end{claim}
\begin{proof}
Докажем для определенности первое утверждение, другое ему симметрично.
Записав все в координатах, мы имеем $\beta \colon F^n \times F^m \to F$ по правилу $\beta(x,y) = x^t By$.
И для левого ядра мы имеем $\ker^L \beta = \{x\in F^n \mid B^t x = 0\}$.
Тогда $\dim \ker^L\beta$ -- количество свободных переменных системы $B^t x=0$ (раздел~\ref{section::Subspaces} о ФСР), $\rk \beta$ -- количество главных переменных системы $B^t x = 0$ (совпадает со строчным рангом $B^t$), а $\dim V $ -- количество переменных системы $B^t x = 0$.
Ну а количество главных плюс количество свободных переменных -- это все переменные.
\end{proof}

\subsection{Двойственность для подпространств}

\begin{claim}
\label{claim::DualitySpaces}
Пусть $\beta\colon V\times U\to F$ -- невырожденная билинейная форма.
Тогда:
\begin{enumerate}
\item Для любого подпространства $W\subseteq V$ выполнено
\[
\dim W^\bot + \dim W = \dim V
\]

\item Для любого подпространства $W\subseteq V$ выполнено ${}^\bot(W^\bot) = W$.

\item Для любых подпространств $W\subseteq E\subseteq V$ верно, что $W^\bot \supseteq E^\bot$.
Причем $W = E$ тогда и только тогда, когда $W^\bot = E^\bot$.

\item Для любых подпространств $W, E\subseteq V$ выполнено равенство
\[
(W + E)^\bot = W^\bot \cap E^\bot
\]

\item Для любых подпространств $W, E\subseteq V$ выполнено равенство
\[
(W\cap E)^\bot = W^\bot + E^\bot
\]
\end{enumerate}
Аналогично выполнены все свойства для подпространств $W\subseteq U$ и их левых ортогональных дополнений ${}^\bot W$.
\end{claim}
\begin{proof}
Давайте прежде всего перейдем в координаты выбрав какой-нибудь базис $V$ и $U$.
Тогда получим $\beta\colon F^n \times F^n \to F$ по правилу $\beta(x,y) = x^t B y$, где $B\in \operatorname{M}_n(F)$ -- невырожденная матрица.

(1) Пусть $W = \langle w_1,\ldots,w_r \rangle$ задано своим базисом и $T = (w_1|\ldots|w_r)\in \operatorname{M}_{n\,r}(F)$.
Тогда $W^\bot = \{y\in F^n \mid T^tB y = 0\}$.
Так как матрица $B$ невырожденная, то $\rk(T^t B) = \rk (T^t) = r$ (утверждение~\ref{claim::rkInvariance}).
В очередной раз все интерпретируем в терминах свободных и главных переменных системы $T^t By = 0$.
Имеем: $\dim W = r$ -- это количество главных переменных системы, $\dim W^\bot$ -- это количество свободных переменных, а $\dim V$ -- это количество всех переменных, что и требовалось.

(2) Давайте в начале покажем, что $W \subseteq {}^\bot(W^\bot)$, а потом сравним их размерности.
Пусть $w\in W$, нам надо показать, что $w\in {}^\bot (W^\bot)$.
То есть нам надо показать, что $\beta(w, W^\bot) = 0$.
То есть для любого $v\in W^\bot$ надо показать, что $\beta(w,v) = 0$.
Однако, по определению, если $v\in W^\bot$, то $\beta(w, v) = 0$ для любого $w\in W$, что и требовалось.
Теперь надо показать, что пространства имеют одинаковую размерность.
Для этого воспользуемся пунктом~(1):
\[
\dim {}^\bot (W^\bot) = \dim U - \dim W^\bot = \dim U- (\dim V - \dim W) = \dim W
\]
последнее равенство в силу того, что $\dim U = \dim V$.
А раз пространства вложены и имеют одинаковую размерность, то они совпадают.

(3) Пусть $W\subseteq E\subseteq V$, тогда
\[
W^\bot = \{u\in U\mid \beta(w, u) = 0,\,w\in W\} \quad \text{и} \quad E^\bot = \{u\in U\mid \beta(e, u) = 0,\,e\in E\}
\]
Заметим, что так как $W\subseteq E$, то справа ограничений не меньше, чем слева, а значит пространство не больше.

Пусть теперь $E, W\subseteq V$ -- произвольные подпространства.
Тогда если они равны, то и их ортогональные дополнения равны.
Обратно, пусть $W^\bot = E^\bot$, тогда ${}^\bot(W^\bot) = {}^\bot(E^\bot)$, то есть по пункту~(2) $W = E$.

(4) Рассмотрим левую и правую части равенства $(W + E)^\bot = W^\bot\cap E^\bot$ отдельно:
\[
(W+E)^\bot = \{u\in U\mid \beta(w + e, u) = 0,\,\forall w\in W,\,\forall e\in E\}
\]
С другой стороны
\begin{gather*}
W^\bot\cap E^\bot = \{u\in U\mid \beta(w, u) = 0,\,\forall w\in W\}\cap \{u\in U\mid \beta(e,u) = 0,\,\forall e\in E\} =\\
\{u\in U\mid \beta(w,u) = 0,\forall w\in W\text{ и }\beta(e,u)=0,\,\forall e\in E\}
\end{gather*}
Если $u\in W^\bot\cap E^\bot$, то $\beta(w,u) = 0$ и $\beta(e,u) = 0$ для любых $w\in W$ и $e\in E$, а значит и $\beta(w + e, u) = 0$, то есть $u\in (W+E)^\bot$.
Обратно, если $u\in (W+E)^\bot$, то $\beta(w+e,u) = 0$ для любых $w\in W$ и $e\in E$.
В частности для любого $w\in W$ и $e = 0\in E$ получаем $\beta(w, u) = 0$, аналогично $w = 0\in W$ и любого $e \in E$ получаем $\beta(e, u) = 0$.
То есть $u\in W^\bot\cap E^\bot$.

(5) Выведем это утверждение из предыдущего с помощью остальных.
Действительно, чтобы доказать равенство $(W\cap E)^\bot = W^\bot + E^\bot$, необходимо и достаточно доказать ${}^\bot((W\cap E)^\bot) = {}^\bot(W^\bot + E^\bot)$ по пункту~(3) вторая часть.
В силу~(2) это равносильно $W\cap E = {}^\bot(W^\bot+E^\bot)$.
По пункту~(4) для левых ортогональных дополнений получаем, что правая часть совпадает с ${}^\bot(W^\bot) \cap {}^\bot(E^\bot)$.
И опять воспользовавшись~(2), получаем $W\cap E$, то есть левую часть.
\end{proof}

К этому утверждению надо относиться так.
Процедура взятия ортогонального дополнения <<переворачивает>> множество подпространств <<вверх ногами>>, меняет размерность на <<коразмерность>>%
\footnote{Для подпространства $U\subseteq V$ его коразмерность -- это $\dim V - \dim U$.}%
, обращает включения и меняет местами сумму и пересечение.
Это один из способов переформулировать задачи про подпространства и сводить одни к другим.
Если у вас есть задача на пересечение подпространств, то перейдя к ортогональным дополнениям, вы получаете эквивалентную задачу на сумму подпространств и решить ее -- то же самое, что решить исходную задачу.
Например, алгоритмы на поиск суммы и пересечения подпространств заданных порождающими, можно превратить в алгоритмы на поиск суммы и пересечения подпространств заданных системами, применив переход к ортогональному дополнению.
