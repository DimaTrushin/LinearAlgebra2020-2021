\ProvidesFile{lecture25.tex}[Лекция 25]


\subsection{Разложение в прямую сумму}

Пусть $V$ -- векторное пространство над полем $F$.
Тогда будем обозначать через $\SBil(V)$ множество симметричных билинейных форм на $V$, а через $\ABil(V)$ множество кососимметричных билинейных форм.
Напомним, что если $2 = 0$ в поле $F$, то $\ABil(V)\subseteq \SBil(V)$ (утверждение~\ref{claim::BilSymAntiSym}).
Этот пример объясняет ограничение случаем $2 \neq 0$ в следующем утверждении.

\begin{claim}
\label{claim::BilDirectSA}
Пусть $V$ -- векторное пространство над полем $F$ таким, что $2 \neq 0$.
Тогда любая билинейная форма $\beta\colon V\times V\to F$ единственным образом раскладывается в сумму симметричной и кососимметричной.
На языке векторных пространств это означает, что $\Bil(V) = \SBil(V) \oplus \ABil(V)$.
\end{claim}
\begin{proof}
Я лишь предъявлю желаемое разложение 
\[
\beta(v,u) = \frac{\beta(v,u) + \beta(u, v)}{2} + \frac{\beta(v,u) - \beta(u,v)}{2}
\]
Все детали остаются на совести читателя.
\end{proof}

В дальнейшем наше основное внимание будет уделено симметричным билинейным формам.


\subsection{Ограничение билинейной формы на подпространство}

\begin{definition}
Пусть $\beta\colon V\times V\to F$ -- билинейная форма и $U\subseteq V$ -- подпространство.
Тогда через $\beta|_U\colon U\times U\to F$ будем обозначать билинейную форму, действующую по правилу $(u_1, u_2)\mapsto \beta(u_1, u_2)$ для всех $u_1,u_2\in U$.
Форма $\beta|_U$ будет называться ограничением $\beta$ на $U$.%
\footnote{В случае формы $\beta\colon V\times U\to F$ также можно определить ограничение, но для этого нужно иметь пару подпространство $V'\subseteq V$ и $U'\subseteq U$.
Полученная форма будет $\beta|_{V'\times U'}\colon V'\times U' \to F$.
Однако мы ими не пользуемся и обозначения у них ужасные.}
\end{definition}

По-простому, ограничение формы -- это та же самая форма, которая забыла как перемножать все векторы из нашего пространства, а помнит только про перемножение векторов из подпространства.


\paragraph{Замечание}

Пусть $\beta\colon V\times V\to F$ -- некоторая билинейная форма и $U\subseteq V$ -- подпространство.
Пусть $e_1,\ldots,e_n$ -- базис $V$ такой, что $e_1,\ldots,e_k$ образуют базис $U$.
Тогда матрица $\beta$ в базисе $e_1,\ldots,e_n$ будет $B_{\beta} = (\beta(e_i, e_j))_{1\leqslant i,j \leqslant n}$.
С другой стороны, матрица $\beta|_U$ в базисе $e_1,\ldots,e_k$ будет $B_{\beta|_U} = (\beta(e_i, e_j))_{1\leqslant i,j \leqslant k}$.
То есть матрица $B_{\beta|_U}$ -- это левый верхний блок размера $k$ в матрице $B_{\beta}$.
Таким образом, в отличие от линейных операторов, матрицу ограничения  билинейной формы очень легко считать.

\begin{claim}
\label{claim::NonDegRestrictionBil}
Пусть $\beta\colon V\times V\to F$ -- билинейная форма и $U\subseteq V$ -- некоторое подпространство.
Тогда
\begin{enumerate}
\item Выполнены следующие равенства
\begin{enumerate}
\item $\ker^L \beta|_U = U\cap {}^\bot U$

\item $\ker^R \beta|_U = U \cap U^\bot$
\end{enumerate}

\item Следующие условия эквивалентны
\begin{multicols}{2}
\begin{enumerate}
\item $\beta|_U$ невырождена

\item $U\cap {}^\bot U = 0$

\item $U \cap U^\bot = 0$

\item $V = U\oplus U^\bot$

\item $V = U\oplus{}^\bot U$

\item[\vspace{\fill}]
\end{enumerate}
\end{multicols}
\end{enumerate}
\end{claim}
\begin{proof}
(1) Этот пункт проверяется по определению.
Я проверю лишь первый.
Имеем
\[
\ker^L \beta|_U = \{u \in U\mid \beta(u, U) = 0\}
\]
С другой стороны
\[
U \cap {}^\bot U = U \cap \{v\in V\mid \beta(v, U) = 0\} = \{u\in U\mid \beta(u, U) = 0\}
\]
Получили одно и то же.

(2) По определению форма не вырождена тогда и только тогда, когда у нее оба ядра ноль.
Из первого пункта следует, что $U\cap {}^\bot U$ и $U\cap U^\bot$ -- это ядра формы $\beta|_U$.
С другой стороны, так как форма действует на одном пространстве, то размерности ядер совпадают (утверждение~\ref{claim::BilinearKernels}).
Таким образом доказана эквивалентность первых трех пунктов.

Понятно, что (d) -- более сильная версия (c) и (e) -- более сильная версия (b).
Остается показать, что (c) влечет (d) и аналогично (b) влечет (e).
Покажем первое.
Так как подпространства $U$ и $U^\bot$ не пересекаются, то они образуют прямую сумму $U\oplus U^\bot \subseteq V$ и надо лишь показать, что в сумме получается все $V$.
Для этого достаточно показать, что $U^\bot$ имеет размерность хотя бы $\dim V - \dim U$.
Пусть $U = \langle v_1,\ldots, v_k\rangle$, тогда 
\[
U^\bot = \{v\in V \mid \beta(v_1, v) = \ldots = \beta(v_k, v) = 0\}
\]
То есть $U^\bot$ задается системой из $k$ уравнений и $n = \dim V$ переменных.
Значит ранг этой системы не превосходит $k$, а количество свободных переменных не меньше $n - k$ и равно размерности $U^\bot$, что и требовалось.
\end{proof}

\paragraph{Замечания}

\begin{itemize}
\item Если $\beta\colon V\times V\to F$ -- билинейная форма и $U =\langle v \rangle \subseteq V$ -- подпространство порожденное одним вектором $v\neq 0$.
Тогда $v$ -- базис $U$ и матрица $\beta|_U$ в этом базисе -- это $\beta(v, v)$.
Потому $\beta|_U$ невырождена тогда и только тогда, когда $\beta(v, v)\neq 0$.

\item Пусть $\beta\colon V\times V\to F$ -- некоторая билинейная форма, $U\subseteq V$ -- некоторое подпространство.
Предположим, что $V = U \oplus {}^\bot U$.
Выберем базис $U = \langle e_1,\ldots,e_k\rangle$ и базис ${}^\bot U = \langle g_{k+1},\ldots, g_n\rangle$.
Тогда в базисе $e_1,\ldots,e_k, g_{k+1}, \ldots, g_n$ матрица $\beta$ имеет вид
\[
\begin{pmatrix}
{B_{\beta|_U}}&{*}\\
{0}&{B_{\beta|_{{}^\bot U}}}
\end{pmatrix}
\]
Если же $V = U \oplus U^\bot$ и $U^\bot =\langle f_{k+1},\ldots, f_n\rangle$, то в базисе $e_1,\ldots,e_k,f_{k+1},\ldots, f_n$ матрица $\beta$ имеет вид
\[
\begin{pmatrix}
{B_{\beta|_U}}&{0}\\
{*}&{B_{\beta|_{U^\bot}}}
\end{pmatrix}
\]
Если же $U^\bot = {}^\bot U$,%
\footnote{Например такое бывает, если $\beta$ симметрична.}
то, выбрав базисы $U = \langle e_1,\ldots,e_k\rangle$ и $U^\bot = {}^\bot U = \langle e_{k+1}, \ldots, e_n\rangle$, мы получим матрицу для $\beta$ вида
\[
\begin{pmatrix}
{B_{\beta|_U}}&{0}\\
{0}&{B_{\beta|_{U^\bot}}}
\end{pmatrix}
\]
Таким образом выделение угла нулей в матрице билинейной формы -- это вопрос разложения пространства $V$ в прямую сумму подпространства $U$ и одного из его двух ортогональных дополнений.
А выделение блочно диагонального вида означает, что надо подобрать такое $U$, чтобы его левое и правое ортогональные дополнения совпали и все пространство $V$ разваливалось в прямую сумму $U$ и дополнения.
\end{itemize}

\subsection{Диагонализация симметричных форм}

\begin{claim}
\label{claim::SBilToDiag}
Пусть $\beta\colon V\times V\to F$ -- симметричная билинейная форма и $2 \neq 0$ в $F$.
Тогда существует такой базис, что матрица формы $\beta$ имеет диагональный вид.
\end{claim}
\begin{proof}
Что значит найти базис $e_1,\ldots,e_n$, в котором матрица $\beta$ будет диагональной? Это значит, найти базис, в котором $\beta(e_i,e_j) = 0$ при $i\neq j$.
Потому план будет следующий: если $\beta \neq 0$, то найдем некоторый вектор $v\in V$ такой, чтобы $V = \langle v \rangle \oplus \langle v \rangle^\bot$.
Положим $e_1 = v$, а векторы $e_2,\ldots,e_n$ выберем по индукции в подпространстве $\langle v \rangle^\bot$ (если $\beta$ нулевая, то годится любой базис).
Полученная система векторов будет ортогональным базисом.
Чтобы завершить доказательство, надо объяснить, почему всегда можно выбрать такой вектор $v$.

Рассмотрим значения $\beta(v,v)$ для всех $v\in V$.
Если это значение всегда ноль, то $\beta$ -- кососимметричная, но она одновременно симметричная.
Так как $2 \neq 0$, такое возможно только если $\beta = 0$.
В этом случае все доказано, матрица $\beta$ будет диагональная в любом базисе.
Значит мы можем предположить, что найдется такой $v\in V$, что $\beta(v, v)\neq 0$.
Последнее означает, что $\beta|_U$ невырождена, где $U = \langle v \rangle$.
В силу утверждения~\ref{claim::NonDegRestrictionBil} это означает, что $U \oplus U^\bot = V$.
Обозначим $e_1 = v$ выберем $e_2,\ldots,e_n\in U^\bot$ из индукционного предположения.
\end{proof}


\paragraph{Замечание}

Предположим, что $2 = 0$ в поле $F$.
Тогда рассмотрим $V = F^2$ и зададим билинейную форму $\beta\colon V\times V\to F$ по правилу $(x,y)\mapsto x^t By$ для матрицы%
\footnote{Заметим, что в силу того, что $1 = -1$ в $F$, то эта матрица еще к тому же и кососимметричная.}
\[
B = 
\begin{pmatrix}
{0}&{1}\\
{1}&{0}
\end{pmatrix}
\]
Если выбрать произвольную матрицу $C \in \operatorname{M}_2(F)$ с неопределенными коэффициентам
\[
C = 
\begin{pmatrix}
{a}&{b}\\
{c}&{d}
\end{pmatrix}
\]
Тогда
\[
C^t B C = 
\begin{pmatrix}
{a}&{c}\\
{b}&{d}
\end{pmatrix}
\begin{pmatrix}
{0}&{1}\\
{1}&{0}
\end{pmatrix}
\begin{pmatrix}
{a}&{b}\\
{c}&{d}
\end{pmatrix}
=
\begin{pmatrix}
{a}&{c}\\
{b}&{d}
\end{pmatrix}
\begin{pmatrix}
{c}&{d}\\
{a}&{b}
\end{pmatrix}
=
\begin{pmatrix}
{0}&{ad + bc}\\
{ad +  bc}&{0}
\end{pmatrix}
=
\det(C)B
\]
В выкладках выше не забывайте, что $1 = -1$ в $F$.
Таким образом, какую бы замену мы ни с делали, матрица $B$ лишь изменится на скаляр из $F$ и никогда не диагонализуется.
Значит в предыдущем утверждении нельзя отбросить предположение $2 \neq 0$.

\paragraph{Симметричный Гаусс}

Теперь, когда мы знаем, что симметричные билинейные формы диагонализуются в каком-то базисе, хорошо было бы иметь какой-нибудь (ну хотя бы плохонький) алгоритм, приводящий форму к диагональному виду, если она задана в каком-то случайном базисе.
Пусть, скажем, нам задана билинейная форма $\beta\colon F^n \times F^n\to F$ по правилу $(x,y)\mapsto x^t By$, где $B\in \operatorname{M}_n(F)$ -- некоторая симметричная матрица.
Тогда в  новом базисе матрица будет иметь вид $C^t B C$, где $C\in \operatorname{M}_n(F)$ -- некоторая невырожденная матрица.%
\footnote{На самом деле $C$ -- матрица перехода из старого в новый базис.}
Любая невырожденная матрица $C$ раскладывается в произведение элементарных матриц (утверждение~\ref{claim::InvertibleDiscription}).
С другой стороны, если $C$ -- матрица элементарного преобразования, то $B \mapsto C^tBC$ -- это выполнение одного и того же преобразования и над строками и над столбцами (не важно в каком порядке, так как произведение матриц ассоциативно).
То есть у нас есть следующий запас операций:
\begin{itemize}
\item Прибавляем $i$-ю строку умноженную на $\lambda$ к $j$-ой строке, потом прибавляем $i$-ый столбец умноженный на $\lambda$ к $j$-ому столбцу.

\item Меняем местами $i$ и $j$ строки, после чего меняем местами $i$ и $j$ столбцы.

\item Умножаем на ненулевое $\lambda$ $i$-ю строку, потом умножаем на $\lambda$ $i$-ый столбец.
\end{itemize}
Таким образом предыдущая теорема гласит, что выполняя подобные симметричные элементарные преобразования над симметричной матрицей, мы обязательно приведем ее к диагональному виду.


\subsection{Метод Якоби}
\label{subsection::Jacoby}

\paragraph{Постановка задачи}

Пусть $\beta\colon V\times V\to F$ -- некоторая симметричная билинейная форма, $e_1,\ldots,e_n$ -- базис пространства $V$ и $B = (\beta(e_i, e_j))$ -- матрица билинейной формы в этом базисе, то есть в базисе $e_1,\ldots,e_n$ билинейная форма задается как $\beta(x, y) = x^t B y$, где $x,y\in F^n$ -- координаты векторов в базисе $e_1,\ldots,e_n$.

Введем следующие обозначения: $U_k = \langle e_1,\ldots,e_k\rangle$  -- подпространство натянутое на первые $k$ векторов исходного базиса.
Теперь выделим в матрице $B$ верхние левые блоки:
\[
B =
\begin{pmatrix}
{\boxed{
\begin{matrix}
{
\boxed{
\begin{matrix}
{
\boxed{
\begin{matrix}
{\boxed{b_{11}}}&{}\\
{}&{\ddots}
\end{matrix}
}
}&{}\\
{}&{B_k}
\end{matrix}
}
}&{}\\
{}&{\ddots}
\end{matrix}
}
}&{}\\
{}&{}
\end{pmatrix}
\]
То есть $B_k$ -- подматрица состоящая из первых $k$ строк и столбцов.
Тогда $B_k$ -- это в точности матрица $\beta|_{U_k}$ в базисе $e_1,\ldots,e_k$.
Так же обозначим через $\Delta_k$ определители $\det (B_k)$.
В дальнейшем мы будем предполагать, что все $\Delta_k \neq 0$ и наша задача будет найти базис $e_1',\ldots,e_n'$ в пространстве $V$ такой, чтоб $\langle e_1',\ldots,e_k'\rangle = U_k$ и $\beta(e_i',e_j') = 0$ при $i\neq j$.


\paragraph{Описание метода}

Предположим теперь, что все $\Delta_k\neq 0$.
Будем строить векторы нового базиса $e_1',\ldots,e_n'$ по следующим рекурентным формулам
\[
\left\{
\begin{aligned}
e_1' &= e_1\\
e_2'  &= e_2 - \frac{\beta(e_2, e_1')}{\beta(e_1',e_1')}e_1'\\
e_3'  &= e_3 - \frac{\beta(e_3, e_1')}{\beta(e_1',e_1')}e_1'- \frac{\beta(e_3, e_2')}{\beta(e_2',e_2')}e_2'\\
&\ldots\\
e_k' &= e_k - \frac{\beta(e_k, e_1')}{\beta(e_1',e_1')}e_1' - \ldots - \frac{\beta(e_k, e_{k-1}')}{\beta(e_{k-1}',e_{k-1}')}e_{k-1}'\\
&\ldots
\end{aligned}
\right.
\]
Наша задача показать, что эти формулы всегда сработают и приведут к нужному результату.
То есть нам надо показать, что все знаменатели вида $\beta(e_i',e_i')$ не равны нулю и все векторы $e_1',\ldots,e_n'$ ортогональны друг другу, то есть $\beta(e_i', e_j') = 0$ при $i\neq j$.
Для того, чтобы доказать это, мы будем индукцией по номеру построенного вектора проверять выполнимость трех инвариантов
\begin{gather*}
\langle e_1,\ldots,e_k\rangle = \langle e_1',\ldots, e_k'\rangle\\
\Delta_k = \beta(e_1',e_1') \ldots \beta(e_k',e_k')\\
\beta(e_i',e_j') = 0\text{ при }i\neq j,\;i,j\leqslant k
\end{gather*}
И уже из этого мы выведем корректность алгоритма.


\begin{claim}
\label{claim::JacobiInvariants}
Пусть $\beta\colon V\times V\to F$ -- симметричная билинейная форма, $e_1,\ldots,e_n$ -- базис пространства $V$ и $B = (\beta(e_i, e_j))$ и все диагональные подматрицы $B_k$ не вырождены, то есть $\Delta_k \neq 0$ для любого $1\leqslant k\leqslant n$.
Предположим, что мы построили $k$ векторов $e_1',\ldots,e_k'$ по методу Якоби описанному выше и при этом выполнено
\begin{gather*}
\langle e_1,\ldots,e_k\rangle = \langle e_1',\ldots, e_k'\rangle\\
\Delta_k = \beta(e_1',e_1') \ldots \beta(e_k',e_k')\\
\beta(e_i',e_j') = 0\text{ при }i\neq j,\;i,j\leqslant k
\end{gather*}
Тогда
\begin{enumerate}
\item Вектор 
\[
e_{k+1}' = e_{k+1} - \frac{\beta(e_{k+1}, e_1')}{\beta(e_1',e_1')}e_1' - \ldots - \frac{\beta(e_{k+1}, e_{k}')}{\beta(e_{k}',e_{k}')}e_{k}'
\]
корректно определен и ортогонален всем векторам $e_1',\ldots,e_k'$.

\item Выполнены равенства
\begin{gather*}
\langle e_1,\ldots,e_{k+1}\rangle = \langle e_1',\ldots, e_{k+1}'\rangle\\
\Delta_{k+1} = \beta(e_1',e_1') \ldots \beta(e_{k+1}',e_{k+1}')\\
\beta(e_i',e_j') = 0\text{ при }i\neq j,\;i,j\leqslant k+1
\end{gather*}
\end{enumerate}
\end{claim}
\begin{proof}
1) Так как $\Delta_k\neq 0$, то из условия $\Delta_k = \beta(e_1',e_1') \ldots \beta(e_k',e_k')$ следует, что все знаменатели в формуле для $e_{k+1}'$ не равны нулю.
Значит $e_{k+1}'$ корректно определен.
Давайте проверим, что он оказался ортогонален $\langle e_1',\ldots,e_k'\rangle = \langle e_1,\ldots,e_k\rangle$.
Для этого посчитаем 
\[
\beta(e_{k+1}', e_i') = \beta(e_{k+1}, e_i') - \frac{\beta(e_{k+1}, e_1')}{\beta(e_1',e_1')}\beta(e_1', e_i') - \ldots - \frac{\beta(e_{k+1}, e_{k}')}{\beta(e_{k}',e_{k}')}\beta(e_{k}', e_i')
\]
Так как все построенные вектры $e_1',\ldots,e_k'$ были ортогональны, то справа выживает лишь одно слагаемое, то есть
\[
\beta(e_{k+1}', e_i') = \beta(e_{k+1}, e_i') - \frac{\beta(e_{k+1}, e_i')}{\beta(e_i',e_i')}\beta(e_i', e_i') = 0
\]

2) По построению $e_{k+1}'-e_{k+1} \in \langle e_1',\ldots,e_k'\rangle = \langle e_1,\ldots,e_k\rangle$.
Откуда получаем, что $\langle e_1,\ldots,e_{k+1}\rangle = \langle e_1',\ldots,e_{k+1}'\rangle$.
Кроме того, все векторы по построению получились ортогональными.
Осталось лишь показать, что $\Delta_{k+1}$ равно произведению $\beta(e_1',e_1')\ldots \beta(e_{k+1}',e_{k+1}')$.
Для этого заметим, что мы в подпространстве $U_{k+1}$ сделали замену базиса по правилам
\[
\begin{pmatrix}
{e_1}&{\ldots}&{e_{k+1}}
\end{pmatrix}
=
\begin{pmatrix}
{e_1'}&{\ldots}&{e_{k+1}'}
\end{pmatrix}
\begin{pmatrix}
{1}&{\frac{\beta(e_2,e_1')}{\beta(e_1',e_1')}}&{\frac{\beta(e_3,e_1')}{\beta(e_1',e_1')}}&{\ldots}&{\frac{\beta(e_{k+1},e_1')}{\beta(e_1',e_1')}}\\
{}&{1}&{\frac{\beta(e_3,e_2')}{\beta(e_2',e_2')}}&{\ldots}&{\frac{\beta(e_{k+1},e_2')}{\beta(e_2',e_2')}}\\
{}&{}&{1}&{}&{}\\
{}&{}&{}&{\ddots}&{\vdots}\\
{}&{}&{}&{}&{1}\\
\end{pmatrix}
\]
Обозначим матрицу справа за $C$.
Тогда это будет матрица перехода от $e_1',\ldots,e_{k+1}'$ к базису $e_1,\ldots,e_{k+1}$.
Пусть $B_k'$ будет матрица $\beta$ в штрихованном базисе.
Тогда $B_{k+1} = C^t B_{k+1}' C$, а значит 
\[
\Delta_{k+1} = \det B_{k+1}  = \det B_{k+1}' \det C^2 = \det B_{k+1} = \beta(e_1',e_1')\ldots \beta(e_{k+1}',e_{k+1}')
\]
Что доказывает оставшееся равенство.
\end{proof}

\paragraph{Замечания}

\begin{enumerate}
\item Утверждение~\ref{claim::JacobiInvariants} показывает, что 
\begin{itemize}
\item Метод Якоби для поиска базиса $e_1',\ldots,e_n'$ сработает на каждом шаге и в итоге мы получим диагональную матрицу $B'$.

\item Полученные диагональные элементы $b_{ii}'$ матрицы $B'$ можно вычислить по формуле
\[
b_{ii}' = \beta(e_i',e_i') = \frac{\Delta_i}{\Delta_{i-1}}
\]
при этом мы считаем, что $\Delta_0 = 1$.

\item Исходная матрица $B$ представляется в виде $B = C^t B' C$, где
\[
C = 
\begin{pmatrix}
{1}&{\frac{\beta(e_2,e_1')}{\beta(e_1',e_1')}}&{\frac{\beta(e_3,e_1')}{\beta(e_1',e_1')}}&{\ldots}&{\frac{\beta(e_{k+1},e_1')}{\beta(e_1',e_1')}}\\
{}&{1}&{\frac{\beta(e_3,e_2')}{\beta(e_2',e_2')}}&{\ldots}&{\frac{\beta(e_{k+1},e_2')}{\beta(e_2',e_2')}}\\
{}&{}&{1}&{}&{}\\
{}&{}&{}&{\ddots}&{\vdots}\\
{}&{}&{}&{}&{1}\\
\end{pmatrix}
\quad
B' =
\begin{pmatrix}
{\Delta_1}&{}&{}&{}\\
{}&{\frac{\Delta_2}{\Delta_1}}&{}&{}\\
{}&{}&{\ddots}&{}\\
{}&{}&{}&{\frac{\Delta_n}{\Delta_{n-1}}}\\
\end{pmatrix}
\]
\end{itemize}

\item Если вы недоумеваете (а вообще говоря очень даже должны) откуда взялись эти дурацкие формулы для векторов $e_k'$ и как вообще можно было до них догадаться, то давайте я приоткрою завесу тайны и сообщу всю правду.
На самом деле мы производили следующий процесс.
Мы взяли вектор $e_1' = e_1$.
Далее мы решили ортогонализовать вектор $e_2$ относительно $e_1'$.
А именно, в плоскости $\langle e_1, e_2\rangle = \langle e_1', e_2\rangle$ вектор $e_2$ представляется как что-то параллельное $e_1'$ и что-то ортогональное $e_1'$.%
\footnote{Например потому что $\langle e_1, e_2\rangle = \langle e_1\rangle \oplus \langle e_1\rangle^\bot$, где ортогональное дополнение берется внутри $\langle e_1,e_2\rangle$.}
Тогда будем искать разложение вида $e_2 = \lambda e_1' + w$.
При этом будем подбирать параметр $\lambda$ так, чтобы $w$ оказался ортогонален $e_1'$, то есть
\[
0 = \beta(w, e_1')  = \beta(e_2, e_1') - \lambda\beta( e_1',e_1')
\]
Отсюда находим формулу для $\lambda$, а $w$ полагаем новым вектором $e_2'$.
Аналогично поступаем на следующем шаге, мы теперь будем пытаться раскладывать вектор $e_2$ в виде
\[
e_2 = \lambda e_1' + \mu e_2' + w
\]
И коэффициенты $\lambda$ и $\mu$ будут искаться из соображений, чтобы $w$ был ортогонален $e_1'$ и $e_2'$.
Применяя $\beta({-}, e_i')$ к $w$ мы находим коэффициенты $\lambda$ и $\mu$, а $w$ полагаем за $e_2'$ и т.д.
\end{enumerate}
