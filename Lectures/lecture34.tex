\ProvidesFile{lecture34.tex}[Лекция 34]


\subsection{Классификация линейных отображений}

Мы с вами уже занимались классификацией линейных отображений между двумя разными пространствами (утверждение~\ref{claim::HomClassification}).
Выбирая базисы в двух пространствах независимо, мы можем добиться того, чтобы матрица превратилась в диагональную с единицами и нулями на диагонали.
По сути, две прямоугольные матрицы задают одно и то же отображение между двумя разными пространствами тогда и только тогда, когда у них одинаковый ранг.
Теперь у нас ситуация немного более жесткая.
У нас теперь есть два пространства $V$ и $U$ и они либо вещественные, либо эрмитовы и задано линейное отображение $\phi\colon V\to U$.
Но теперь в отличие от общего случая я хочу выбирать не произвольные базисы, а только ортонормированные.
Таким образом у меня меньше свободы для модификации матрицы оператора.
Оказывается, что даже в таком жестком случае, можно сделать матрицу оператора диагональной, то вот на диагонали будут стоять произвольные неотрицательные числа.

\begin{claim}
\label{claim::HermEuclHomClass}
Пусть $V$ и $U$ -- евклидовы или эрмитовы пространства и $\phi\colon V\to U$ -- линейное отображение.
Тогда
существует ортонормированный базис $e_1,\ldots,e_n$ в $V$, ортонормированный базис $f_1,\ldots,f_m$ в $U$ и последовательность вещественных чисел $\sigma_1\geqslant \sigma_2 \geqslant \ldots \geqslant \sigma_r > 0$ такие, что матрица $\phi$ имеет вид
\[
\phi(e_1,\ldots,e_n) = (f_1,\ldots,f_m) 
\begin{pmatrix}
{\sigma_1}&{}&{}&{}&{}&{}\\
{}&{\ddots}&{}&{}&{}&{}\\
{}&{}&{\sigma_r}&{}&{}&{}\\
{}&{}&{}&{\ddots}&{}&{}\\
{}&{}&{}&{}&{0}&{}\\
\end{pmatrix}
\]
При этом числа $\sigma_1,\ldots,\sigma_r$ определены однозначно и называются сингулярными значениями отображения $\phi$.
\end{claim}
\begin{proof}
Давайте рассмотрим билинейную форму $\beta(v, u) = (\phi(v), \phi(u))$ на пространстве $V$.
Тогда это симметричная (эрмитова) неотрицательно определенная форма.
Значит по утверждению~\ref{claim::BilinOrthoDiag} существует ортонормированный базис $e_1,\ldots,e_n$ в $V$, в котором она диагональна.
Пусть матрица $\beta$ в этом базисе имеет вид $\diag(\lambda_1,\ldots,\lambda_r,0\ldots,0)$, причем $\lambda_1\geqslant \ldots \geqslant \lambda_r>0$.
Положим $\sigma_1 = \sqrt{\lambda_1},\ldots,\sigma_r = \sqrt{\lambda_r}$.
Теперь определим векторы $f_i = \frac{1}{\sigma_i}\phi(e_i)$ для $1\leqslant i\leqslant r$.
Покажем, что система векторов $(f_1,\ldots,f_r)$ является ортономированной.
Сначала проверим длины
\[
(f_i, f_i) = \left(\frac{1}{\sigma_i}\phi(e_i), \frac{1}{\sigma_i} \phi(e_i)\right) = \frac{1}{\sigma_i^2}\beta(e_i, e_i) = \frac{\lambda_i}{\sigma_i^2} = 1
\]
Теперь проверим ортогональность
\[
(f_i, f_j) = \left(\frac{1}{\sigma_i}\phi(e_i), \frac{1}{\sigma_j} \phi(e_j)\right) = \frac{1}{\sigma_i\sigma_j}\beta(e_i, e_j) = 0
\]
Теперь дополним векторы $f_1,\ldots,f_r\in U$ до ортонормированного базиса пространства $U$ и получим $f_1,\ldots,f_m$.
Теперь методом пристального взгляда проверяем, что построенная пара базисов удовлетворяет требуемому условию:
\[
\phi(e_1,\ldots,e_n) = (f_1,\ldots,f_m) 
\begin{pmatrix}
{\sigma_1}&{}&{}&{}&{}&{}\\
{}&{\ddots}&{}&{}&{}&{}\\
{}&{}&{\sigma_r}&{}&{}&{}\\
{}&{}&{}&{\ddots}&{}&{}\\
{}&{}&{}&{}&{0}&{}\\
\end{pmatrix}
\]

Осталось показать, что числа $\sigma_i$ определены однозначно.
Заметим, что $\beta(v, u) = (\phi v, \phi u) = (v, \phi^* \phi u)$.
Тогда числа $\sigma_i$ определены как корни из спектра оператора $\phi^*\phi$.
\end{proof}

\subsection{SVD или сингулярное разложение}

Давайте переформулируем последнее утверждение в матричных терминах.
Чтобы упростить изложение, я все сделаю для вещественного случая.
Комплексный делается аналогично с той лишь разницей, что везде в формулах транспонированную матрицу $M^t$ надо менять на эрмитово сопряженную $M^* = \bar M^t$ при любой матрице $M$.
В начале я переформулирую утверждение на матричном языке.

\begin{claim}
Пусть дана матрица $A\in \MatrixDim{m}{n}$.
Тогда \begin{enumerate}
\item Существует $U\in \Matrix{m}$ такая, что $U^t U = E$.

\item Существует $V\in \Matrix{n}$ такая, что $V^t V = E$.

\item Существует последовательность вещественных чисел $\sigma_1\geqslant \sigma_2\geqslant \ldots\geqslant \sigma_r > 0$.
\end{enumerate}
такие, что $A = U \Sigma V^t$, где
\[
\Sigma =
\overbrace{
\begin{pmatrix}
{\sigma_1}&{}&{}&{}&{}&{}\\
{}&{\ddots}&{}&{}&{}&{}\\
{}&{}&{\sigma_r}&{}&{}&{}\\
{}&{}&{}&{\ddots}&{}&{}\\
{}&{}&{}&{}&{0}&{}\\
\end{pmatrix}
}^n
\left.
\vphantom{
\begin{pmatrix}
{\sigma_1}&{}&{}&{}&{}&{}\\
{}&{\ddots}&{}&{}&{}&{}\\
{}&{}&{\sigma_r}&{}&{}&{}\\
{}&{}&{}&{\ddots}&{}&{}\\
{}&{}&{}&{}&{0}&{}\\
\end{pmatrix}
}
\right\}{\scriptstyle m}
\]
При этом последовательность чисел $\sigma_1,\sigma_2,\ldots,\sigma_r$ определена однозначно.
\end{claim}

\begin{itemize}
\item Разложение матрицы $A$ в этом утверждении называется сингулярным разложением или SVD.

\item
Чтобы свести матричное утверждение к оператоному, надо рассмотреть пространства $\mathbb R^n$ и $\mathbb R^m$ со стандартным скалярным произведением и оператор $\phi\colon \mathbb R^n \to \mathbb R^m$ по правилу $x\mapsto Ax$.
Тогда столбцы матрицы $U$ -- это базисные векторы $f_1,\ldots,f_m\in \mathbb R^m$, а столбцы $V$ -- это базисные векторы $e_1,\ldots,e_n$ из утверждения~\ref{claim::HermEuclHomClass}.

\item
Обратите внимание, что такое разложение не единственное.
Например, если $A$ квадратная и равна единичной матрице, то подойдет любое разложение вида $A = U E U^t$, где $U$ -- произвольная ортогональная матрица.
В данном случае эффект связан с тем, что все сингулярные значения одинаковые.
Но даже, если сингулярные значения разные, вы можете домножать соответствующие столбцы $U$ и $V$ на минус единицу,%
\footnote{В комплексном случае домножать можно на пару сопряженных чисел по модулю равных единице.}
например.
\[
A = (u_1|\ldots | u_m)\Sigma (v_1|\ldots|v_n)^t
\quad\text{тогда}\quad
A = (-u_1|\ldots | u_m)\Sigma (-v_1|\ldots|v_n)^t
\]

\item
Последний пример показывает, что не только разложение неоднозначно, но и что между столбцами $U$ и $V$ есть некоторое условие согласованности.
Из-за этой неоднозначности в разложении, надо аккуратно искать все компоненты этого разложения.
Если вы нашли какие-то части разложения, например матрицы $U$ и $\Sigma$, то матрицу $V$ надо искать не абы каким методом.
\end{itemize}

Заметим, что в SVD матрица $\Sigma$ имеет тот же размер, что и матрица $A$.
Однако, ее правая часть полностью заполнена нулями.
А значит, что умножение этой части на соответствующую часть матрица $V^t$ ни на что не повлияет.
Предположим для определенности, что $n > m$, как на картинках.
Тогда можем определить матрицу $\Sigma_0\in \Matrix{m}$ полученную из $\Sigma$ отрезанием последних $n - m$ нулевых столбцов.
Так же заменим матрицу $V = (v_1|\ldots|v_n)$ на матрицу $V_0 = (v_1|\ldots|v_m)$.
Тогда получим разложение вида 
\[
A = U \Sigma_0 V_0^t
\]
Это разложение называется усеченным сингулярным разложением и многие алгоритмы ищут именно его.
Здесь квадратными являются $U$ и $\Sigma_0$, а столбцы $V_0$ образуют ортонормированную систему (не обязательно базис).
Если бы было $m > n$, то разложение было бы вида
\[
A = U_0 \Sigma_0 V^t
\]
при этом квадратными были бы матрицы $\Sigma_0$ и $V$, а столбцы матрицы $U_0$ образуют ортонормированную систему (не обязательно базис).


Если пойти еще дальше и применить блочные формулы к сингулярному разложению, то можно получить разложение матрицы $A$ в сумму матриц ранга $1$, а именно
\[
A = \sigma_1 u_1 v_1^t + \ldots + \sigma_r u_r v_r^t
\]
Если задать на пространстве матриц $\MatrixDim{m}{n}$ скалярное произведение в виде $(A, B) = \tr(A^t B)$, то матрицы $u_iv_i^t$ будут ортонормированной системой в пространстве матриц, а написанное выше разложение -- это разложение матрицы $A$ по ортонормированному базису, состоящему из матриц ранга $1$.

Последнее разложение используется для сжатия информации с потерей информации.
Например, про матрицу $A$ можно думать как про картинку в шкале серого, где в ячейке матрицы задана интенсивность черного.
В реальной жизни, числа $\sigma_1\geqslant \ldots \geqslant \sigma_r$ убывают очень быстро и последние значения ничтожно малы.
Если вы откините последнее слагаемое $\sigma_ru_rv_r^t$, то вы измените исходную матрицу $A$.
Однако, $u_r$ и $v_r$ -- векторы нормы $1$, а значит их координаты по модулю не больше единицы, а значит у матрицы $\sigma_r u_rv_r^t$ координаты не больше, чем $\sigma_r$ по модулю.
Значит, если $\sigma_r$  мало, то выкидывание последнего слагаемого меняет каждый коэффициент матрицы $A$ на очень малую величину, незаметную для глаза.
Таким образом, вместо того, чтобы хранить матрицу $A$ целиком (для этого нужно $mn$ ячеек памяти), мы можем отрезать в каком-нибудь месте сумму в сингулярном разложении и хранить
\[
A' = \sigma_1 u_1 v_1^t + \ldots + \sigma_k u_k v_k^t
\]
Здесь нам понадобится $k$ ячеек памяти под $\sigma_i$, $km$ ячеек памяти под векторы $u_i$ и $kn$ ячеек памяти под векторы $v_i$.
Итого будет $k(1 + n + m)$ ячеек памяти.
При этом параметр $k$ контролирует степень сжатия картинки.

\subsection{Ортопроекторы}

В этом разделе я хочу обсудить некоторые технические факты, которые мне понадобятся для задачи о низкоранговом приближении.
Давайте напомню, что такое проекторы и ортопроекторы.%
\footnote{Мы уже изучили их в разделе~\ref{section::LinearOpDef} и разделе~\ref{section::OrthoProjection}.}
Пусть $V$ -- векторное пространство, $U, W\subseteq V$ -- его подпространства и $V = U \oplus W$.
Тогда любой вектор $ v$ однозначно раскладывается в сумму $v = u + w$, где $u\in U$ и $w\in W$.
Тогда можно определить отображение $P\colon V\to V$ по правилу $v\mapsto u$.
Как легко видеть $P$ является линейным оператором.
Он называется проектором на $U$ вдоль $W$.
Заметим, что $P$ действует тождественно на $U$ и нулем на $W$.
Кроме того, $U = \Im P$, а $W = \ker P$.
Или в терминах собственных подпространств $U$ -- это $V_1$ собственное подпространство для $1$, а $W = V_0$ -- собственное подпространство для $0$.
Кроме того, мы с вами уже знаем, что оператор $P\colon V\to V$ является проектором тогда и только тогда, когда $P^2 = P$ (утверждение~\ref{claim::Projector}).

Пусть теперь $V$ -- евклидово или эрмитово пространство и $U\subseteq V$ -- некоторое подпространство.
Тогда $V = U\oplus U^\bot$.
В этом случае проектор на $U$ вдоль $U^\bot$ называется ортопроектором на $U$.

\begin{claim}
Пусть $V$ -- евклидово или эрмитово пространство и $P\colon V\to V$ -- некоторый оператор.
Тогда $P$ является ортопроектором тогда и только тогда, когда $P^2 = P$ и $P^* = P$.
\end{claim}
\begin{proof}
Мы уже знаем, что $P$ проектор тогда и только тогда, когда $P^2 = P$.
Надо показать, что при этом условии ядро и образ проектора ортогональны тогда и только тогда, когда $P$ самосопряжен.

Предположим, что $P$ ортопроектор, то есть ядро ортогонально образу.
Выберем $e_1,\ldots,e_k$ -- ортонормированный базис образа и $e_{k+1}, \ldots,e_n$ -- ортонормированный базис ядра.
Тогда в силу ортогональности ядра и образа, $e_1,\ldots,e_n$ будет ортонормированным базисом $V$ и в нем $P$ задан матрицей $A=\left(\begin{smallmatrix}{E}&{0}\\{0}&{0}\end{smallmatrix}\right)$.
То есть $A^t = A$ в вещественном случае или $\bar A^t  = A$ в комплексном.
Последнее означает, что $P$ самосопряжен.

Обратно, пусть $P^* = P$.
Надо показать, что ядро и образ ортогональны.
Пусть $u = Pv\in \Im P$ и $w \in \ker P$, тогда
\[
(u, w) = (Pv, w) = (v, P^*w) = (v, Pw) = (v, 0) = 0
\]
\end{proof}

\begin{claim}
\label{claim::BasisProj}
Пусть $V$ -- евклидово или эрмитово пространство, $P\colon V\to V$ -- ортопроектор на некоторое подпространство $U$ и $e_1,\ldots,e_n$ -- ортонормированный базис пространства $V$.
Тогда
\[
\sum_{i=1}^n |Pe_i|^2 = \dim U
\]
\end{claim}
\begin{proof}
Давайте посчитаем след оператора $P$ двумя разными способами.
Если мы выберем базис $e_1,\ldots,e_k$ в пространстве $U$ и $e_{k+1}, \ldots,e_n$ базис в $U^\bot$, то оператор $P$ в нем задается матрицей $\left(\begin{smallmatrix}{E}&{0}\\{0}&{0}\end{smallmatrix}\right)$.
То есть $\tr P = \dim U$.

С другой стороны, давайте посчитаем левую часть выражения
\[
\sum_{i=1}^n |Pe_i|^2 = \sum_{i=1}^n (Pe_i, Pe_i) = \sum_{i=1}^n (e_i, P^*Pe_i) = \sum_{i=1}^n (e_i, P^2e_i) = \sum_{i=1}^n (e_i, Pe_i)
\]
Так как базис $e_1,\ldots,e_n$ ортонормированный, то скалярное произведение стандартное.
Если $P(e_1,\ldots,e_n) = (e_1,\ldots,e_n)A$, то $(e_i, Pe_i) = a_{ii}$.
Таким образом $ \sum_{i=1}^n (e_i, Pe_i) = \sum_{i=1}^na_{ii} = \tr P$.
\end{proof}

\subsection{Задача о низкоранговом приближении}
\label{section::Approx}

Пусть теперь $A\in \MatrixDim{m}{n}$ или $A\in \operatorname{M}_{m\,n}(\mathbb C)$.
Зададим на пространствах матриц скалярное произведение.
В вещественном случае по формуле $(A, B) = \tr(A^t B)$, а в комплексном $(A, B) = \tr(A^*B)$.
Длина относительно заданного скалярного произведения называется нормой фробениуса и выражается следующим образом (в вещественном или комплексном случае соответственно):
\[
\|A\|_F = \sqrt{\sum_{ij}a_{ij}^2}
\quad\text{или}\quad
\|A\|_F = \sqrt{\sum_{ij}|a_{ij}|^2}
\]
Если матрица $A$ имеет вид $A = (A_1|\ldots|A_n)$, тогда $\|A\|_F^2 = \sum_{i=1}^n |A_i|^2$, где $|A_i|$ -- длина относительно стандартного скалярного произведения для столбца $A_i$.
В силу последнего замечания легко видеть, что норма фробениуса не меняется при домножении матрицы слева или справа на ортогональную матрицу.

Теперь наша задача -- заменить матрицу $A$ на матрицу $B$ ранга не выше $k$, причем мы хотим выбрать $B$ ближайшей в смысле нормы фробениуса.
То есть мы зафиксируем матрицу $A$ и число $k$ и будем решать задачу
\[
\left\{
\begin{aligned}
&\|A - B\|_F \to \min_B\\
&\rk B \leqslant k
\end{aligned}
\right.
\]
Важно понимать, что множество матриц ранга не выше $k$ не образуют линейное подпространство в пространстве матриц.
А значит, тут не получится решить эту задачу просто применением ортопроекторов.
Кроме того, задача может иметь не единственное решение, в некоторых ситуациях ближайших матриц может оказаться бесконечное число.

Обратите внимание, что если $k \geqslant \rk A$, то ответом будет сама матрица $A$.
А если $k < \rk A$, то оказывается, что SVD дает нужный ответ к данной задаче.
Нужно найти для матрицы $A$ сингулярное разложение.
После чего, выбрать в качестве нужной матрицы матрицу
\[
B_k = \sigma_1 u_1 v_1^t + \ldots + \sigma_k u_k v_k^t
\]
Доказательству этого факта будет посвящен этот раздел.

\begin{claim}
Пусть $A\in\MatrixDim{m}{n}$, $A = U \Sigma V^t$ -- ее сингулярное разложение и $\sigma_1\geqslant \sigma_2\geqslant \ldots \geqslant \sigma_r$ -- ее сингулярные числа.
Определим матрицу
\[
\Sigma_k =
\begin{pmatrix}
{\sigma_1}&{}&{}&{}&{}\\
{}&{\ddots}&{}&{}&{}\\
{}&{}&{\sigma_k}&{}&{}\\
{}&{}&{}&{0}&{}\\
\end{pmatrix}
\in \MatrixDim{m}{n}
\]
Тогда матрица $B_k = U\Sigma_k V^t$ будет ближайшей матрицей ранга не более $k$ к матрице $A$ по норме Фробениуса, то есть $\|A - B_k\|_F\leqslant \|A - B\|_F$ для любой матрицы $B\in \MatrixDim{m}{n}$ ранга не более $k$.
\end{claim}
\begin{proof}
Так как у нас есть претендент на минимум, то мы для начала посчитаем значение $\|A-B_k\|_F$.
Получим
\[
\|A-B_k\|_F^2 = \|U(\Sigma - \Sigma_k)V^t\|_F^2 =  \|\Sigma - \Sigma_k\|_F^2
\]
Последнее равенство выполняется в силу того, что $U$ и $V$ ортогональные (см.~замечание в начале раздела~\ref{section::Approx}).
Но матрица $\Sigma-\Sigma_k$ является диагональная с $\sigma_{k+1},\ldots,\sigma_r$ на диагонали.
Потому
\[
\|A-B_k\|_F^2  = \sigma_{k+1}^2 +\ldots + \sigma_r^2
\]
То есть нам теперь надо доказать, что для любой $B\in \MatrixDim{m}{n}$ с условием $\rk B\leqslant k$ выполнено
\[
\|A - B\|_F^2\geqslant \sigma_{k+1}^2 +\ldots + \sigma_r^2
\]
В начале сведем случай к диагональной матрице $A$ следующим образом.
\[
\|A - B\|_F = \|U \Sigma V^t - B\|_F = \|\Sigma - U^t B V\|_F = \|\Sigma - B'\|_F
\]
где $B' = U^t B V$.
Введем обозначения для столбцов матрицы $B' = (B_1|\ldots|B_n)$ и пусть $e_1,\ldots,e_m$ -- стандартный базис в $\mathbb R^m$.
Тогда
\[
 \|\Sigma - B'\|^2_F = \Bigl\| (\sigma_1 e_1|\ldots|\sigma_r e_r|0 |\ldots|0) - (B_1|\ldots|B_n)\Bigl\|^2_F = \sum_{i=1}^r |\sigma_i e_i - B_i|^2 + \sum_{i=r+1}^n|B_i|^2
\]
Теперь оценим это выражение снизу отбросив вторую сумму, получим
\[
 \|\Sigma - B'\|^2_F\geqslant  \sum_{i=1}^r |\sigma_i e_i - B_i|^2
\]
При этом обратите внимание, что если линейная оболочка $U = \langle B_1,\ldots, B_n\rangle $ не лежит в линейной оболочке столбцов матрицы $\Sigma$, то у нас строгое неравенство.
Величина $|\sigma_i e_i - B_i|$ не меньше, чем расстояние от вектора $\sigma_i e_i$ до $U$.
Значит мы можем оценить эту разность через ортогональную составляющую и написать
\[
 \sum_{i=1}^r |\sigma_i e_i - B_i|^2 \geqslant \sum_{i=1}^r |\ort_U(\sigma_i e_i)|^2 =  \sum_{i=1}^r \sigma^2_i| \ort_U(e_i)|^2 
\]
Теперь обозначим $|\ort_U(e_i)|^2 = t_i$ (здесь $1\leqslant i \leqslant m$).
По утверждению~\ref{claim::BasisProj} имеем $\sum_{i=1}^r t_i = \dim \Im \ort_U \geqslant m - k$.
Теперь наша задача показать, что
\[
\sum_{i=1}^t \sigma_i^2 t_i\geqslant \sum_{i = k+1}^r\sigma_i^2\quad\text{при условии}\quad
\left\{
\begin{aligned}
&0 \leqslant t_i \leqslant 1\\
&\sum_{i=1}^r t_i \geqslant m - k
\end{aligned}
\right.
\]
Так как все функции неравенствах линейные и $\sigma_i^2$ упорядочены по убыванию, то мы извлекаем ответ методом пристального взгляда.
А именно, минимум будет в случае
\[
t_1 = 0,\ldots,t_k = 0, t_{k+1} = 1,\ldots,t_m = 1
\]
В этом случае минимальное значение выражения $\sum_{i=1}^t \sigma_i^2 t_i$ как раз получается $ \sum_{i = k+1}^r\sigma_i^2$.
\end{proof}

\paragraph{Замечания}

 Обратите внимание на одну тонкость.
 В утверждении выше, мы показали, что матрица $B_k$ построенная по сингулярному разложению подходит в качестве минимума выражения $\|A - B\|_F$ среди матриц ранга $k$.
 Однако, мы с вами не знаем, а есть ли другие минимумы.
 Оказывается, что этот вопрос можно исследовать и ситуация следующая:
\begin{enumerate}
\item Пусть $A = U \Sigma V^t$ -- сингулярное разложение некоторой матрицы $A$ и $\sigma_k = \sigma_{k+1}$.
Тогда матрица $B_k$ ранга $k$, на которой достигается минимум $\|A - B\|_F$, не единственная.
Это можно проверить построив два разных сингулярных разложения, но я не буду тут этим заниматься.

\item Пусть $A = U \Sigma V^t$ -- сингулярное разложение некоторой матрицы $A$ и $\sigma_k > \sigma_{k+1}$.
Тогда матрица $B_k$ ранга $k$, на которой достигается минимум $\|A - B\|_F$, единственная.
Делается это приблизительно так: мы сначала сводим задачу к диагональной матрице $A$, а потом надо явно найти градиент выражения $\|A - B\|_F^2$, рассматриваемого как функцию от $B$.
Красиво это можно сделать с помощью матричных дифференцирований.
Но в результате получится, что $B$ должна коммутировать с диагональной матрицей $A$.
И следовательно наша задача разваливается в несколько независимых оптимизационных задач, анализируя которые, можно увидеть единственность точки минимума.
Мучить этим доказательством я вас не буду, но знать этот факт полезно.
\end{enumerate}
